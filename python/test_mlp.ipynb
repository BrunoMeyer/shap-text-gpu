{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c9fe00",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "# reproducibility\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a5d39ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters (script defaults)\n",
    "DATASET = \"imdb\"\n",
    "TOKENIZER = \"bert-base-uncased\"\n",
    "MAX_LEN = 64\n",
    "HIDDEN_DIM = 128\n",
    "NUM_LAYERS = 3\n",
    "TRAIN_SAMPLES = 2000\n",
    "TEST_SAMPLES = 256\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Validation / early stopping\n",
    "VAL_RATIO = 0.1\n",
    "PATIENCE = 5\n",
    "# Dropout (applied between layers during training only)\n",
    "DROPOUT_RATE = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5403c47b",
   "metadata": {},
   "source": [
    "# Reproduce default MLP from `train_export.py`\n",
    "This notebook splits parameter definition, imports, dataset prep, tokenization, training, and testing into separate cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea28728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding-based MLP: word-level embeddings + mean pooling\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import vocab as torchtext_vocab\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "class EmbeddingMLP(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, hidden_dim: int, num_classes: int, padding_idx: int):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n",
    "        self.hidden_fc = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.out_fc = nn.Linear(hidden_dim, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.padding_idx = padding_idx\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (batch, seq_len) of token ids (torch.long)\n",
    "        emb = self.embedding(x)  # (batch, seq_len, embed_dim)\n",
    "        mask = (x != self.padding_idx).unsqueeze(-1).to(emb.dtype)  # (batch, seq_len, 1)\n",
    "        summed = (emb * mask).sum(dim=1)  # (batch, embed_dim)\n",
    "        lengths = mask.sum(dim=1).clamp(min=1)  # (batch, 1)\n",
    "        mean_pooled = summed / lengths  # (batch, embed_dim)\n",
    "        h = self.relu(self.hidden_fc(mean_pooled))\n",
    "        return self.out_fc(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "405b8e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset loading and subsampling\n",
    "ds = load_dataset(DATASET)\n",
    "train_split = ds['train']\n",
    "test_split = ds['test'] if 'test' in ds else ds['train']\n",
    "train_n = min(TRAIN_SAMPLES, len(train_split))\n",
    "test_n = min(TEST_SAMPLES, len(test_split))\n",
    "def stratified_select(dataset, n, label_field='label', seed=123):\n",
    "    labels = list(dataset[label_field])\n",
    "    total = len(labels)\n",
    "    if n >= total:\n",
    "        return dataset.shuffle(seed=seed)\n",
    "    classes = sorted(set(labels))\n",
    "    counts = {c: labels.count(c) for c in classes}\n",
    "    # Allocate per-class counts proportionally, with at least 1 when possible\n",
    "    alloc = {}\n",
    "    remaining = n\n",
    "    for c in classes[:-1]:\n",
    "        k = max(1, int(round(counts[c] / total * n)))\n",
    "        k = min(k, counts[c])\n",
    "        alloc[c] = k\n",
    "        remaining -= k\n",
    "    last = classes[-1]\n",
    "    alloc[last] = min(counts[last], max(0, remaining))\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    selected = []\n",
    "    for c in classes:\n",
    "        idxs = [i for i, lab in enumerate(labels) if lab == c]\n",
    "        random.shuffle(idxs)\n",
    "        take = alloc.get(c, 0)\n",
    "        selected.extend(idxs[:take])\n",
    "    # If short due to rounding, fill from remaining indices\n",
    "    if len(selected) < n:\n",
    "        remaining_idxs = [i for i in range(total) if i not in selected]\n",
    "        random.shuffle(remaining_idxs)\n",
    "        selected.extend(remaining_idxs[:(n - len(selected))])\n",
    "    random.shuffle(selected)\n",
    "    return dataset.select(selected)\n",
    "# Apply stratified subsampling to train and test splits\n",
    "train_split = stratified_select(train_split, train_n, seed=123)\n",
    "test_split = stratified_select(test_split, test_n, seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b23d656f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This two-parter was excellent - the best since the series returned. Sure bits of the story were pinched from previous films, but what TV shows don\\'t do that these days. What we got here was a cracking good sci-fi story. A great big (really scary) monster imprisoned at the base of a deep pit, some superb aliens in The Ood - the best \"new\" aliens the revived series has come up with, a set of basically sympathetic and believable human characters (complete with a couple of unnamed \"expendable\" security people in true Star Trek fashion), some large-scale philosophical themes (love, loyalty, faith, etc.), and some top-drawer special effects.<br /><br />I loved every minute of this.',\n",
       " \"Stan Laurel and Oliver Hardy had extensive (separate) film careers before they were eventually teamed. For many of Ollie's pre-Stan films, he was billed on screen as Babe Hardy ... and throughout his adult life, Hardy was known to his friends as 'Babe'. While touring postwar Britain with Laurel in a music-hall act for Bernard Delfont, Hardy gave an interview to journalist John McCabe in which he explained the origin of this nickname: early in his acting career, Hardy got a shave from a gay hairdresser who squeezed Hardy's plump cheeks (the ones on his face) and said 'Nice baby!' Hardy's workmates started crying him 'Babe', and the nickname stuck.<br /><br />Although much of Hardy's pre-Laurel work is very interesting -- notably his comedy roles in support of Larry Semon and the Chaplin imitator Billy West -- his teamwork with Billy Ruge (who?) in a series of low-budget shorts for the Vim Comedy Film Company is very dire indeed. Hardy and Ruge were given the screen names Plump and Runt: names which are unpleasant in their own right, but made worse because Ruge (although shorter than Hardy) isn't especially a runt. Seen here, Hardy looks much as he does in his early Hal Roach films with Laurel ... but without the spit curls and the fastidious little moustache.<br /><br />'One Too Many', an absolutely typical Plunt and Runt epic, is direly unfunny ... and its dreichness is made even more conspicuous by the fact that this film has exactly the same premise as 'That's My Wife', one of Laurel and Hardy's most hilarious films. Plump (Hardy) is the star boarder in a rooming-house run by a tall gawky landlady. Runt (Ruge) is the porter. Plump receives a letter from his wealthy uncle John, whose dosh he expects to inherit. His uncle is coming to see him and to meet Plump's wife and baby. There's only one problem: Plump hasn't got a wife and baby. He's been lying to his uncle in order to seem a family man. Now, of course, Plump expects Runt to find him a wife and baby on short notice. Of course, the results are disastrous. It would be nice if those disastrous results were funny, but they aren't. Most of the unfunny humour here is just empty slapstick, with characters settling their arguments by shoving each other into bathtubs.<br /><br />SPOILERS COMING. Vim director Will Louis (who?) shows no instinct for camera framing: the actress who plays the landlady is significantly taller than Hardy, and Louis consistently sets up his shots so that her head is out of frame. This could be funny if done on purpose, but it's merely inept. At one point in this bad comedy, an extremely tasteless gag is looming on the horizon as Runt approaches a black laundress. 'Surely they wouldn't stoop THAT low for a laugh,' I thought. But they do. Runt steals the woman's black infant and tries to fob this off as Plump's progeny.<br /><br />Somehow, Plump acquires an infant's cot, but he still hasn't got a baby. With Uncle John coming up the stairs, Plump conscripts Runt for babyhood. This gag might just possibly have worked with a midget, or even with a truly runt-sized actor such as Chester Conklin, but Billy Ruge is only slightly below average height. Ruge's impersonation of a baby is neither believable nor funny, and Uncle John would have to be a complete moron to fall for it. Amazingly, he does!<br /><br />The most notable aspect of 'One Too Many' is a brief appearance -- apparently her only-ever film appearance -- by Madelyn Saloshin, Oliver Hardy's first wife. The marriage was not a happy one, although Hardy's marital troubles never attained the epic proportions of Stan Laurel's. <br /><br />Only one thing in this movie impressed me. There is a very brief flashback sequence, with Hardy reminiscing about his seaside romance with a bathing beauty. In 1916, there was still not yet a standard film grammar for conveying flashbacks: the one shown here is done gracefully and simply. Too bad this movie has no other merits. 'One Too Many' is definitely one film too many on Oliver Hardy's CV, and I'll rate this movie just one point out of 10. Laurel and Hardy together are definitely much funnier than either of them separately.\",\n",
       " 'My scalp still smarts from the burning coals heaped on it when I vowed I love this film. Bring on the coals; I\\'ll walk over them as well to say again that I love \"Bend it Like Beckham.\" Granted, there\\'s a lot of \"in spite of\" in that confession. It\\'s a bit movie-of-the-week; the screenplay is on the paint-by-numbers side. And, most troublingly, the director\\'s commentary implies that in this film beauty can be found primarily amongst the white of skin.<br /><br />The film\\'s genius is not in what\\'s obvious to the Syd Field-doctored eye: character arcs, themes, construction. It\\'s in both the surface and what lurks deep beneath, but not in those layers of artistic topsoil that reviewers seem most often to scratch at. Powerful, sometimes semi-clad female bodies not simply on display but kicking the crap out of a football do a better job of naturalizing female strength and agility than Lara Croft or Zhang Ziyi will ever do. These are real bodies (Keira Knightley\\'s excepted) whose work is not to look great first and kick butt later. They are working bodies whose beauty is in their movement and self-determination. And, in my book, lead actress Parminder Nagra is one of the most gorgeous creatures ever captured on screen \\x96 not only because she can lay claim to that hackneyed adjective, \"luminous,\" but because her performance has an honesty and un-bookish intelligence that\\'s utterly compelling.<br /><br />The result is a film women can enjoy without feeling like they\\'re making a pact with the devil to do so. As in Chadha\\'s \"Bride and Prejudice,\" the relationships amongst women sizzle with a chemistry that can\\'t be neatly slotted into the stodgy, Sweet Valley High categories of \"best friends\" or \"sisters.\" Perhaps Chadha is even right in her commentary to disavow the film\\'s flirtation with lesbianism. \"Bend it Like Beckham\" has an electricity that can\\'t be reduced to the simple hetero/homosexual love triangle its conventionally structured script would suggest. The precise nature of its pleasure is, ultimately, a bit of a mystery \\x96 and is all the more seductive for it.<br /><br />Oh yes, and did I mention that it\\'s hilarious?',\n",
       " \"Featured in 1955's THE COBWEB is an all star cast ranging from silent screen veteran LILLIAN GISH to Actors Studio progeny SUSAN STRASBERG. Set at an exclusive psychiatric hospital, what is this movie about you wonder......high drama ? Doctor & patient relationships ? Shock therapy treatment ? No, this howler is about who exactly will get to pick the draperies for a psychiatric hospital ! You think I'm kidding ? You won't believe your eyes as you're watching this unbelievable storyline that was turned into a movie ! Progressive head shrink Dr. McIver (RICHARD WIDMARK) wants to have all of the hospital's patients involved in the design, selection and execution of the needed new draperies. McIver's wife played by marble mouthed GLORIA GRAHAM wants to get her 2 cents in on this monumental task too. So does long time staffer Miss Inch (LILLIAN GISH). Directed by VINCENT MINELLI, you kinda wonder if he really became this overly involved in minute detail because of his marriage to worry wart JUDY GARLAND. Talented actors like LAUREN BACALL, SUSAN STRASBERG, CHARLES BOYER, and JOHN KERR are wasted in this hokey story. What were they thinking ?\",\n",
       " \"A well put together entry in the serial killer genre that unfortunately gets mired down in its own pretentiousness to be really satisfying. Willem Dafoe is superb as a NYC detective trying to track down what appears to be a copycat using the same Renaissance art-related killing techniques used in a series of murders he solved years earlier. Scott Speedman is Dafoe's junior partner and they have pretty good chemistry (at least for a while). Other characters pop up to conveniently tie the two cases together. Clea Duval is the friend of an earlier victim and Peter Stormare is some sort of art broker/mentor to Dafoe...that's a bit hard to take, although Stormare is, of course, never dull. The film's ending is particularly disappointing. Look fast for Deborah Harry as Dafoe's less than forthcoming neighbor.\"]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_split['text'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8ce6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 256/256 [00:00<00:00, 8119.77 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenization and tensor conversion using torchtext (word-level)\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import vocab as torchtext_vocab\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "# Build vocabulary from training texts\n",
    "counter = Counter()\n",
    "for txt in train_split['text']:\n",
    "    counter.update(tokenizer(txt))\n",
    "specials = [\"<pad>\", \"<unk>\"]\n",
    "vocab = torchtext_vocab(counter, specials=specials)\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "pad_idx = vocab['<pad>']\n",
    "unk_idx = vocab['<unk>']\n",
    "vocab_size = len(vocab)\n",
    "def tok_batch(batch):\n",
    "    ids_batch = []\n",
    "    for text in batch['text']:\n",
    "        toks = tokenizer(text)\n",
    "        ids = [vocab[t] for t in toks][:MAX_LEN]  # truncate to MAX_LEN\n",
    "        ids_batch.append(ids)\n",
    "    return {'input_ids': ids_batch}\n",
    "train_tok = train_split.map(tok_batch, batched=True, remove_columns=[c for c in train_split.column_names if c != 'label'])\n",
    "test_tok = test_split.map(tok_batch, batched=True, remove_columns=[c for c in test_split.column_names if c != 'label'])\n",
    "def pad_id_lists(id_lists, max_len=MAX_LEN):\n",
    "    tensors = [torch.tensor(ids, dtype=torch.long)[:max_len] for ids in id_lists]\n",
    "    if len(tensors) == 0:\n",
    "        return torch.empty((0, max_len), dtype=torch.long)\n",
    "    padded = pad_sequence(tensors, batch_first=True, padding_value=pad_idx)\n",
    "    if padded.size(1) < max_len:\n",
    "        padded = torch.nn.functional.pad(padded, (0, max_len - padded.size(1)), value=pad_idx)\n",
    "    else:\n",
    "        padded = padded[:, :max_len]\n",
    "    return padded\n",
    "train_ids = pad_id_lists(train_tok['input_ids'], MAX_LEN)\n",
    "train_y = torch.tensor(train_tok['label'], dtype=torch.int64)\n",
    "test_ids = pad_id_lists(test_tok['input_ids'], MAX_LEN)\n",
    "test_y = torch.tensor(test_tok['label'], dtype=torch.int64)\n",
    "num_classes = int(train_y.max().item() + 1)\n",
    "input_dim = int(train_ids.shape[1])  # seq_len (MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4181e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1 train_loss=0.6948 val_acc=0.505\n",
      "epoch=2 train_loss=0.6919 val_acc=0.505\n",
      "epoch=3 train_loss=0.6898 val_acc=0.56\n",
      "epoch=4 train_loss=0.6877 val_acc=0.52\n",
      "epoch=5 train_loss=0.6819 val_acc=0.535\n",
      "epoch=6 train_loss=0.6761 val_acc=0.555\n",
      "epoch=7 train_loss=0.6731 val_acc=0.525\n",
      "epoch=8 train_loss=0.6628 val_acc=0.515\n",
      "Early stopping after 8 epochs (no improvement for 5 epochs)\n"
     ]
    }
   ],
   "source": [
    "# Training with validation and early stopping\n",
    "model = EmbeddingMLP(vocab_size=vocab_size, embed_dim=HIDDEN_DIM, hidden_dim=HIDDEN_DIM, num_classes=num_classes, padding_idx=pad_idx).to(DEVICE)\n",
    "# Create train/val split from train_ids/train_y\n",
    "n_train_total = train_ids.shape[0]\n",
    "val_n = int(n_train_total * VAL_RATIO)\n",
    "if val_n > 0:\n",
    "    # Stratified split to balance classes in train/val\n",
    "    classes = torch.unique(train_y)\n",
    "    train_idx_list = []\n",
    "    val_idx_list = []\n",
    "    for c in classes:\n",
    "        idx_c = (train_y == c).nonzero(as_tuple=True)[0]\n",
    "        idx_c = idx_c[torch.randperm(idx_c.size(0))]\n",
    "        n_val_c = max(1, int(idx_c.size(0) * VAL_RATIO))\n",
    "        val_idx_list.append(idx_c[:n_val_c])\n",
    "        train_idx_list.append(idx_c[n_val_c:])\n",
    "    val_idx = torch.cat(val_idx_list) if len(val_idx_list) > 0 else torch.tensor([], dtype=torch.long)\n",
    "    train_idx = torch.cat(train_idx_list) if len(train_idx_list) > 0 else torch.tensor([], dtype=torch.long)\n",
    "    # Shuffle final indices\n",
    "    train_idx = train_idx[torch.randperm(train_idx.size(0))] if train_idx.numel() > 0 else train_idx\n",
    "    val_idx = val_idx[torch.randperm(val_idx.size(0))] if val_idx.numel() > 0 else val_idx\n",
    "    train_ds = TensorDataset(train_ids[train_idx], train_y[train_idx])\n",
    "    val_ds = TensorDataset(train_ids[val_idx], train_y[val_idx])\n",
    "else:\n",
    "    train_ds = TensorDataset(train_ids, train_y)\n",
    "    val_ds = None\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False) if val_ds is not None else None\n",
    "test_loader = DataLoader(TensorDataset(test_ids, test_y), batch_size=BATCH_SIZE, shuffle=False)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "best_val_acc = -1.0\n",
    "best_state = None\n",
    "epochs_no_improve = 0\n",
    "model.train()\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    running = 0.0\n",
    "    seen = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        yb = yb.to(DEVICE)\n",
    "        optim.zero_grad(set_to_none=True)\n",
    "        logits = model(xb)\n",
    "        loss = loss_fn(logits, yb)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        running += float(loss.item()) * int(yb.numel())\n",
    "        seen += int(yb.numel())\n",
    "    # Validation\n",
    "    val_acc = None\n",
    "    if val_loader is not None:\n",
    "        model.eval()\n",
    "        total_v = 0\n",
    "        correct_v = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(DEVICE)\n",
    "                yb = yb.to(DEVICE)\n",
    "                logits = model(xb)\n",
    "                pred = logits.argmax(dim=-1)\n",
    "                correct_v += int((pred == yb).sum().item())\n",
    "                total_v += int(yb.numel())\n",
    "        val_acc = correct_v / max(total_v, 1)\n",
    "    train_loss = running / max(seen, 1)\n",
    "    # Check improvement on validation (or test if no val set)\n",
    "    monitor_acc = val_acc if val_acc is not None else 0.0\n",
    "    improved = (monitor_acc > best_val_acc)\n",
    "    if improved:\n",
    "        best_val_acc = monitor_acc\n",
    "        # save CPU copy of state dict\n",
    "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "    print(f'epoch={epoch} train_loss={train_loss:.4f} val_acc={val_acc if val_acc is not None else \"N/A\"}')\n",
    "    model.train()\n",
    "    if epochs_no_improve >= PATIENCE:\n",
    "        print(f'Early stopping after {epoch} epochs (no improvement for {PATIENCE} epochs)')\n",
    "        break\n",
    "# restore best model state if available\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d62d5215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_test_acc=0.5273\n"
     ]
    }
   ],
   "source": [
    "# Final testing/evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        yb = yb.to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        correct += int((pred == yb).sum().item())\n",
    "        total += int(yb.numel())\n",
    "    print(f'final_test_acc={correct/max(total,1):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c776b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 9006.61 examples/s]\n",
      "Map: 100%|██████████| 256/256 [00:00<00:00, 8165.40 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config={'lr': 0.001, 'weight_decay': 0.0, 'dropout': 0.3, 'num_layers': 3, 'hidden_dim': 64, 'max_len': 64, 'padding': True} best_val=0.5050 test_acc=0.4805 elapsed=0.7s\n",
      "config={'lr': 0.0005, 'weight_decay': 0.001, 'dropout': 0.0, 'num_layers': 2, 'hidden_dim': 128, 'max_len': 64, 'padding': True} best_val=0.5500 test_acc=0.4727 elapsed=1.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 9153.88 examples/s]\n",
      "Map: 100%|██████████| 256/256 [00:00<00:00, 8157.27 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config={'lr': 0.0005, 'weight_decay': 0.001, 'dropout': 0.5, 'num_layers': 1, 'hidden_dim': 128, 'max_len': 128, 'padding': True} best_val=0.4800 test_acc=0.5312 elapsed=1.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 8863.83 examples/s]\n",
      "Map: 100%|██████████| 256/256 [00:00<00:00, 7706.80 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config={'lr': 0.001, 'weight_decay': 0.001, 'dropout': 0.3, 'num_layers': 1, 'hidden_dim': 128, 'max_len': 32, 'padding': True} best_val=0.4700 test_acc=0.5273 elapsed=2.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 9023.68 examples/s]\n",
      "Map: 100%|██████████| 256/256 [00:00<00:00, 7673.59 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config={'lr': 0.001, 'weight_decay': 0.001, 'dropout': 0.0, 'num_layers': 2, 'hidden_dim': 256, 'max_len': 128, 'padding': 'max_length'} best_val=0.5050 test_acc=0.4922 elapsed=3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 7874.30 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config={'lr': 0.001, 'weight_decay': 0.0, 'dropout': 0.5, 'num_layers': 4, 'hidden_dim': 64, 'max_len': 128, 'padding': True} best_val=0.5000 test_acc=0.5000 elapsed=3.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 8691.76 examples/s]\n",
      "Map: 100%|██████████| 256/256 [00:00<00:00, 7807.67 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config={'lr': 0.001, 'weight_decay': 0.0001, 'dropout': 0.2, 'num_layers': 3, 'hidden_dim': 256, 'max_len': 64, 'padding': 'max_length'} best_val=0.5000 test_acc=0.5234 elapsed=4.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 8318.21 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config={'lr': 0.01, 'weight_decay': 0.0001, 'dropout': 0.0, 'num_layers': 2, 'hidden_dim': 256, 'max_len': 64, 'padding': True} best_val=0.5050 test_acc=0.5391 elapsed=5.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 8528.30 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config={'lr': 0.0005, 'weight_decay': 0.0001, 'dropout': 0.3, 'num_layers': 3, 'hidden_dim': 256, 'max_len': 128, 'padding': 'max_length'} best_val=0.5200 test_acc=0.5000 elapsed=6.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 8914.81 examples/s]\n",
      "Map: 100%|██████████| 256/256 [00:00<00:00, 7639.35 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config={'lr': 0.0005, 'weight_decay': 0.0001, 'dropout': 0.3, 'num_layers': 3, 'hidden_dim': 64, 'max_len': 32, 'padding': 'max_length'} best_val=0.5150 test_acc=0.4961 elapsed=6.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 9076.63 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config={'lr': 0.01, 'weight_decay': 0.0, 'dropout': 0.2, 'num_layers': 3, 'hidden_dim': 128, 'max_len': 64, 'padding': True} best_val=0.5100 test_acc=0.5352 elapsed=7.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 256/256 [00:00<00:00, 7398.53 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config={'lr': 0.0005, 'weight_decay': 0.0001, 'dropout': 0.5, 'num_layers': 3, 'hidden_dim': 64, 'max_len': 128, 'padding': 'max_length'} best_val=0.4900 test_acc=0.4922 elapsed=8.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 8897.60 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config={'lr': 0.005, 'weight_decay': 0.001, 'dropout': 0.5, 'num_layers': 1, 'hidden_dim': 64, 'max_len': 32, 'padding': True} best_val=0.4950 test_acc=0.4531 elapsed=8.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 8858.49 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config={'lr': 0.0005, 'weight_decay': 0.0001, 'dropout': 0.2, 'num_layers': 2, 'hidden_dim': 64, 'max_len': 32, 'padding': True} best_val=0.5000 test_acc=0.4688 elapsed=9.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 9143.74 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config={'lr': 0.0005, 'weight_decay': 0.0001, 'dropout': 0.5, 'num_layers': 1, 'hidden_dim': 128, 'max_len': 64, 'padding': True} best_val=0.5150 test_acc=0.5430 elapsed=10.0s\n",
      "config={'lr': 0.0005, 'weight_decay': 0.0001, 'dropout': 0.3, 'num_layers': 1, 'hidden_dim': 256, 'max_len': 64, 'padding': True} best_val=0.5150 test_acc=0.5430 elapsed=10.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 8657.22 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config={'lr': 0.01, 'weight_decay': 0.0001, 'dropout': 0.2, 'num_layers': 4, 'hidden_dim': 256, 'max_len': 32, 'padding': True} best_val=0.5000 test_acc=0.5000 elapsed=11.3s\n",
      "config={'lr': 0.001, 'weight_decay': 0.001, 'dropout': 0.0, 'num_layers': 3, 'hidden_dim': 128, 'max_len': 32, 'padding': True} best_val=0.5200 test_acc=0.5000 elapsed=11.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 9191.62 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config={'lr': 0.001, 'weight_decay': 0.001, 'dropout': 0.3, 'num_layers': 3, 'hidden_dim': 256, 'max_len': 64, 'padding': 'max_length'} best_val=0.5100 test_acc=0.5273 elapsed=12.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 8759.55 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config={'lr': 0.01, 'weight_decay': 0.0001, 'dropout': 0.3, 'num_layers': 4, 'hidden_dim': 128, 'max_len': 128, 'padding': 'max_length'} best_val=0.5000 test_acc=0.5000 elapsed=13.3s\n",
      "\n",
      "Top results:\n",
      "{'config': {'lr': 0.0005, 'weight_decay': 0.0001, 'dropout': 0.5, 'num_layers': 1, 'hidden_dim': 128, 'max_len': 64, 'padding': True}, 'best_val': 0.515, 'test_acc': 0.54296875, 'elapsed_s': 9.984415054321289}\n",
      "{'config': {'lr': 0.0005, 'weight_decay': 0.0001, 'dropout': 0.3, 'num_layers': 1, 'hidden_dim': 256, 'max_len': 64, 'padding': True}, 'best_val': 0.515, 'test_acc': 0.54296875, 'elapsed_s': 10.53002119064331}\n",
      "{'config': {'lr': 0.01, 'weight_decay': 0.0001, 'dropout': 0.0, 'num_layers': 2, 'hidden_dim': 256, 'max_len': 64, 'padding': True}, 'best_val': 0.505, 'test_acc': 0.5390625, 'elapsed_s': 5.276341676712036}\n",
      "{'config': {'lr': 0.01, 'weight_decay': 0.0, 'dropout': 0.2, 'num_layers': 3, 'hidden_dim': 128, 'max_len': 64, 'padding': True}, 'best_val': 0.51, 'test_acc': 0.53515625, 'elapsed_s': 7.445921421051025}\n",
      "{'config': {'lr': 0.0005, 'weight_decay': 0.001, 'dropout': 0.5, 'num_layers': 1, 'hidden_dim': 128, 'max_len': 128, 'padding': True}, 'best_val': 0.48, 'test_acc': 0.53125, 'elapsed_s': 1.6985588073730469}\n",
      "Saved sweep_results.json\n"
     ]
    }
   ],
   "source": [
    "# Expanded hyperparameter/configuration sweep (sampled from full grid)\n",
    "import itertools, random, json, time\n",
    "# parameter grids\n",
    "lrs = [1e-2, 5e-3, 1e-3, 5e-4]\n",
    "wds = [0.0, 1e-4, 1e-3]\n",
    "dropouts = [0.0, 0.2, 0.3, 0.5]\n",
    "num_layers_list = [1, 2, 3, 4]\n",
    "hidden_dims = [64, 128, 256]\n",
    "max_lens = [32, 64, 128]\n",
    "padding_modes = [True, 'max_length']\n",
    "# sampling configuration\n",
    "FULL_GRID = list(itertools.product(lrs, wds, dropouts, num_layers_list, hidden_dims, max_lens, padding_modes))\n",
    "random.seed(123)\n",
    "random.shuffle(FULL_GRID)\n",
    "N_CONFIGS = min(20, len(FULL_GRID))  # pick a sampled subset to keep runtime reasonable\n",
    "SELECTED = FULL_GRID[:N_CONFIGS]\n",
    "EPOCHS_SWEEP = 20\n",
    "PATIENCE_SWEEP = 5\n",
    "results = []\n",
    "\n",
    "def prepare_tensors_for_config(max_len, padding_mode):\n",
    "    # re-tokenize train/test splits for this config (word-level using existing vocab)\n",
    "    def tok_batch_local(batch):\n",
    "        ids_batch = []\n",
    "        for text in batch['text']:\n",
    "            toks = tokenizer(text)\n",
    "            ids = [vocab[t] for t in toks][:max_len]\n",
    "            ids_batch.append(ids)\n",
    "        return {'input_ids': ids_batch}\n",
    "    tr_tok = train_split.map(tok_batch_local, batched=True, remove_columns=[c for c in train_split.column_names if c != 'label'])\n",
    "    te_tok = test_split.map(tok_batch_local, batched=True, remove_columns=[c for c in test_split.column_names if c != 'label'])\n",
    "    def pad_id_lists_local(id_lists, max_len=max_len):\n",
    "        tensors = [torch.tensor(ids, dtype=torch.long)[:max_len] for ids in id_lists]\n",
    "        if len(tensors) == 0:\n",
    "            return torch.empty((0, max_len), dtype=torch.long)\n",
    "        padded = pad_sequence(tensors, batch_first=True, padding_value=pad_idx)\n",
    "        if padded.size(1) < max_len:\n",
    "            padded = torch.nn.functional.pad(padded, (0, max_len - padded.size(1)), value=pad_idx)\n",
    "        else:\n",
    "            padded = padded[:, :max_len]\n",
    "        return padded\n",
    "    tr_ids = pad_id_lists_local(tr_tok['input_ids'])\n",
    "    te_ids = pad_id_lists_local(te_tok['input_ids'])\n",
    "    tr_y = torch.tensor(tr_tok['label'], dtype=torch.int64)\n",
    "    te_y = torch.tensor(te_tok['label'], dtype=torch.int64)\n",
    "    return tr_ids, tr_y, te_ids, te_y\n",
    "\n",
    "start_time = time.time()\n",
    "for (lr, wd, drop, nlayers, hdim, mlen, pad_mode) in SELECTED:\n",
    "    torch.manual_seed(123)\n",
    "    # prepare tensors for this config\n",
    "    tr_x_cfg, tr_y_cfg, te_x_cfg, te_y_cfg = prepare_tensors_for_config(mlen, pad_mode)\n",
    "    # stratified train/val split (same logic as above)\n",
    "    n_train_total = tr_x_cfg.shape[0]\n",
    "    val_n = int(n_train_total * VAL_RATIO)\n",
    "    if val_n > 0:\n",
    "        classes = torch.unique(tr_y_cfg)\n",
    "        train_idx_list = []\n",
    "        val_idx_list = []\n",
    "        for c in classes:\n",
    "            idx_c = (tr_y_cfg == c).nonzero(as_tuple=True)[0]\n",
    "            idx_c = idx_c[torch.randperm(idx_c.size(0))]\n",
    "            n_val_c = max(1, int(idx_c.size(0) * VAL_RATIO))\n",
    "            val_idx_list.append(idx_c[:n_val_c])\n",
    "            train_idx_list.append(idx_c[n_val_c:])\n",
    "        val_idx = torch.cat(val_idx_list) if len(val_idx_list) > 0 else torch.tensor([], dtype=torch.long)\n",
    "        train_idx = torch.cat(train_idx_list) if len(train_idx_list) > 0 else torch.tensor([], dtype=torch.long)\n",
    "        train_idx = train_idx[torch.randperm(train_idx.size(0))] if train_idx.numel() > 0 else train_idx\n",
    "        val_idx = val_idx[torch.randperm(val_idx.size(0))] if val_idx.numel() > 0 else val_idx\n",
    "        train_ds = TensorDataset(tr_x_cfg[train_idx], tr_y_cfg[train_idx])\n",
    "        val_ds = TensorDataset(tr_x_cfg[val_idx], tr_y_cfg[val_idx])\n",
    "    else:\n",
    "        train_ds = TensorDataset(tr_x_cfg, tr_y_cfg)\n",
    "        val_ds = None\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False) if val_ds is not None else None\n",
    "    # build model for config (embedding-based)\n",
    "    model_s = EmbeddingMLP(vocab_size=vocab_size, embed_dim=hdim, hidden_dim=hdim, num_classes=num_classes, padding_idx=pad_idx).to(DEVICE)\n",
    "    optim_s = torch.optim.Adam(model_s.parameters(), lr=lr, weight_decay=wd)\n",
    "    loss_fn_s = nn.CrossEntropyLoss()\n",
    "    best_val = -1.0\n",
    "    best_state = None\n",
    "    epochs_no_improve = 0\n",
    "    for epoch in range(1, EPOCHS_SWEEP+1):\n",
    "        model_s.train()\n",
    "        running = 0.0\n",
    "        seen = 0\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            yb = yb.to(DEVICE)\n",
    "            optim_s.zero_grad(set_to_none=True)\n",
    "            logits = model_s(xb)\n",
    "            loss = loss_fn_s(logits, yb)\n",
    "            loss.backward()\n",
    "            optim_s.step()\n",
    "            running += float(loss.item()) * int(yb.numel())\n",
    "            seen += int(yb.numel())\n",
    "        # validation\n",
    "        val_acc = None\n",
    "        if val_loader is not None:\n",
    "            model_s.eval()\n",
    "            total_v = 0\n",
    "            correct_v = 0\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in val_loader:\n",
    "                    xb = xb.to(DEVICE)\n",
    "                    yb = yb.to(DEVICE)\n",
    "                    pred = model_s(xb).argmax(dim=-1)\n",
    "                    correct_v += int((pred == yb).sum().item())\n",
    "                    total_v += int(yb.numel())\n",
    "            val_acc = correct_v / max(total_v, 1)\n",
    "        train_loss = running / max(seen, 1)\n",
    "        monitor_acc = val_acc if val_acc is not None else 0.0\n",
    "        if monitor_acc > best_val:\n",
    "            best_val = monitor_acc\n",
    "            best_state = {k: v.cpu().clone() for k, v in model_s.state_dict().items()}\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        if epochs_no_improve >= PATIENCE_SWEEP:\n",
    "            break\n",
    "    if best_state is not None:\n",
    "        model_s.load_state_dict(best_state)\n",
    "    # test evaluation\n",
    "    model_s.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in DataLoader(TensorDataset(te_x_cfg, te_y_cfg), batch_size=BATCH_SIZE):\n",
    "            xb = xb.to(DEVICE)\n",
    "            yb = yb.to(DEVICE)\n",
    "            pred = model_s(xb).argmax(dim=-1)\n",
    "            correct += int((pred == yb).sum().item())\n",
    "            total += int(yb.numel())\n",
    "    test_acc = correct / max(total, 1)\n",
    "    elapsed = time.time() - start_time\n",
    "    cfg = dict(lr=lr, weight_decay=wd, dropout=drop, num_layers=nlayers, hidden_dim=hdim, max_len=mlen, padding=pad_mode)\n",
    "    results.append({'config': cfg, 'best_val': best_val, 'test_acc': test_acc, 'elapsed_s': elapsed})\n",
    "    print(f\"config={cfg} best_val={best_val:.4f} test_acc={test_acc:.4f} elapsed={elapsed:.1f}s\")\n",
    "# summarize top results by test accuracy\n",
    "results_sorted = sorted(results, key=lambda x: x['test_acc'], reverse=True)\n",
    "print('\\nTop results:')\n",
    "for r in results_sorted[:5]:\n",
    "    print(r)\n",
    "# persist results\n",
    "with open('sweep_results.json', 'w') as fh:\n",
    "    json.dump(results_sorted, fh, indent=2)\n",
    "print('Saved sweep_results.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
